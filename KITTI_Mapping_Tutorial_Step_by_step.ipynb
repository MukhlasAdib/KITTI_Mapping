{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KITTI Mapping Tutorial Step-by-step.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "iderI-dPJ7QJ",
        "uVQYbI6wAbdv",
        "y2sVVXDiAfw5",
        "3QTNtHYfCs2e",
        "1aTnCvzTGGO-",
        "zhy-xR-xKPu6",
        "k5HB34TQc8bC",
        "84BWoo6ft0Pc",
        "FwA7TdDCtnL8",
        "RgO7kAzqz8Av",
        "RZzVVZF47bRb",
        "wKHWHNSzIhTR",
        "XrJZJfSx8UXT",
        "1NsVJKtwH1fg",
        "I2ThKoH26A_D",
        "jAJb3jyM6RSt",
        "XrmbUStU6RSt",
        "PZhB3ouU6RSu",
        "R9J6VTpplV_u",
        "VItwuWo2ljyK",
        "gYeJ8xxUlo_J",
        "2LWBhx2Yl_-k"
      ],
      "authorship_tag": "ABX9TyPynQCxDbKQ9nM7CepCEnZC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MukhlasAdib/KITTI_Mapping/blob/main/KITTI_Mapping_Tutorial_Step_by_step.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYlYRBayIP1p"
      },
      "source": [
        "# References\n",
        "\n",
        "[1] Repository for this tutorial: https://github.com/MukhlasAdib/KITTI_Mapping.\n",
        "\n",
        "[2] The full KITTI datased can be accessed here: http://www.cvlibs.net/datasets/kitti/.\n",
        "\n",
        "[3] KITTI Dataset paper: A. Geiger, P. Lenz, C. Stiller and R. Urtasun, \"Vision meets Robotics: The KITTI Dataset,\" *International Journal of Robotics Research (IJRR)*, vol. 32, no. 11, pp. 1231-1237 2013.\n",
        "\n",
        "[4] Description of Occupancy Grid Map (OGM) estimation: Z. Luo, M. V. Mohrenschilt and S. Habibi, \"A probability occupancy grid based approach for real-time LiDAR ground segmentation,\" *IEEE Transactions on Intelligent Transportation Systems*, vol 21, no. 3, pp. 998–1010, Mar. 2020.\n",
        "\n",
        "[5] Description of Dynamic Grid Map (DGM) estimation: J. Moras, V. Cherfaoui and P. Bonnifait, \"Credibilist occupancy grids for vehicle perception in dynamic environments,\" *2011 IEEE International Conference on Robotics and Automation*, Shanghai, China, 2011, pp. 84-89.\n",
        "\n",
        "[6] Paper of DeepLab v3+ for image segmentation: L. C. Chen, Y. Zhu, G. apandreou, F. Schroff and H. Adam, “Encoder-decoder with atrous separable convolution for semantic image segmentation,” *ECCV 2018 Lecture Notes in Computer Science*, vol. 11211, pp. 833–851, 2018.\n",
        "\n",
        "[7] DeepLab v3+ paper via arXiv: https://arxiv.org/abs/1802.02611.\n",
        "\n",
        "[8] DeepLab v3+ repository: https://github.com/tensorflow/models/tree/master/research/deeplab.\n",
        "\n",
        "[9] This tutorial use pykitti module to load the KITTI dataset: https://github.com/utiasSTARS/pykitti."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iderI-dPJ7QJ"
      },
      "source": [
        "# PREPARATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lDKfTLg5WQo"
      },
      "source": [
        "!git clone https://github.com/MukhlasAdib/KITTI_Mapping.git\n",
        "!pip install pykitti"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBjbCBkk6gI1"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import pykitti\n",
        "import tensorflow as tf\n",
        "from sklearn.linear_model import RANSACRegressor\n",
        "from scipy import stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SC0LVdNh66r6"
      },
      "source": [
        "### Load KITTI Data\n",
        "basedir = 'KITTI_Mapping/raw_data/'\n",
        "date = '2011_09_26'\n",
        "drive = '0013'\n",
        "data = pykitti.raw(basedir, date, drive)\n",
        "\n",
        "### Index of data used for test\n",
        "### We use two data from different time\n",
        "idx = 50\n",
        "idx1 = idx+10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDiDJTa7J-fB"
      },
      "source": [
        "# PERCEPTION\n",
        "\n",
        "The goal of the perception system is to extract the information about the round where the vehicle is operating on. This road information will be used to filter out LiDAR points that hit the road so that it can be used for mapping purpose. To extract the information about where the road is, we use deep learning-based image segmentation technique that will be applied to the camera image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVQYbI6wAbdv"
      },
      "source": [
        "## Get Callibration Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoBXqa6l2u8a"
      },
      "source": [
        "Two callibrated parameters that we need:\n",
        "- LiDAR to camera extrinsic matrix - The matrix (4x4) that will be used to transform the LiDAR points to the camera 3D coordinate frame.\n",
        "-  Camera intrinsic matrix - The mastrix (3x3) that will be used to calculate the coordinate of pixels that representat 3D points in camera coordinate.\n",
        "\n",
        "The calibrated parameters are already provided in the dataset. For details, please visit [this explanation](https://www.mathworks.com/help/vision/ug/camera-calibration.html) from Mathworks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-24HbwND-fbH"
      },
      "source": [
        "### Retrieve the provided calibration data\n",
        "lidar2cam_extrinsic = data.calib.T_cam2_velo\n",
        "camera_intrinsic = data.calib.K_cam2\n",
        "\n",
        "print('Lidar to camera extrinsic matrix: ')\n",
        "print(lidar2cam_extrinsic)\n",
        "print()\n",
        "print('Camera intrinsic matrix: ')\n",
        "print(camera_intrinsic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2sVVXDiAfw5"
      },
      "source": [
        "## Load Camera and LiDAR Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTycCXXRACXM"
      },
      "source": [
        "Coordinate system of\n",
        "- Camera = x: right, y: down, z: forward\n",
        "- LiDAR = x: forward, y: left, z: up\n",
        "\n",
        "In this tutorial, camera coordinate system is used. Therefore, the LiDAR points need to be transformed to the camera coordinate frame. Use the LiDAR to camera extrinsic matrix!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kacf9ugO_XGU"
      },
      "source": [
        "def load_data(data,idx):\n",
        "  ### Get the image data\n",
        "  img_raw = np.array(data.get_cam2(idx))\n",
        "\n",
        "  ### Get the LiDAR data (only x,y,z data)\n",
        "  ### Only use LiDAR points that are below the sensor\n",
        "  ### Only use LiDAR points that are at least 2.5 m away\n",
        "  lidar_raw = np.array(data.get_velo(idx))[:,:3]\n",
        "  lidar_raw = lidar_raw[lidar_raw[:,2]<=0,:]\n",
        "  dist = np.linalg.norm(lidar_raw,axis=1)\n",
        "  lidar_raw = lidar_raw[dist >= 2.5]\n",
        "  return img_raw,lidar_raw\n",
        "\n",
        "### Transform the LiDAR points into camera coordinate\n",
        "def transform_coordinate(lidar_points,extrinsic_matrix):\n",
        "  inp = lidar_points.copy()\n",
        "  inp = np.concatenate((inp,np.ones((inp.shape[0],1))),axis=1)\n",
        "  inp = np.matmul(extrinsic_matrix,inp.T).T\n",
        "  return inp[:,:3]\n",
        "\n",
        "img_raw,lidar_raw = load_data(data,idx)\n",
        "img_raw_size = img_raw.shape\n",
        "lidar_raw = transform_coordinate(lidar_raw,lidar2cam_extrinsic)\n",
        "\n",
        "img_raw1,lidar_raw1 = load_data(data,idx1)\n",
        "img_raw1_size = img_raw1.shape\n",
        "lidar_raw1 = transform_coordinate(lidar_raw1,lidar2cam_extrinsic)\n",
        "\n",
        "### Visualize\n",
        "fig,axs = plt.subplots(2,2,figsize=(15,12))\n",
        "axs[0,0].imshow(img_raw)\n",
        "axs[0,1].scatter(lidar_raw[:,0],lidar_raw[:,2],c=-lidar_raw[:,1],marker='.')\n",
        "axs[0,1].scatter(0,0,c='r',marker='x')\n",
        "axs[0,1].axis('scaled')\n",
        "axs[1,0].imshow(img_raw1)\n",
        "axs[1,1].scatter(lidar_raw1[:,0],lidar_raw1[:,2],c=-lidar_raw1[:,1],marker='.')\n",
        "axs[1,1].scatter(0,0,c='r',marker='x')\n",
        "axs[1,1].axis('scaled')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QTNtHYfCs2e"
      },
      "source": [
        "## Project the LiDAR Points to the Camera"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ytg6gknC6nYY"
      },
      "source": [
        "To transfer the road information from image to LiDAR, we need to project the LiDAR points to the camera image. Use the camera intrinsic matrix!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwSf1GLfCr1T"
      },
      "source": [
        "def project_lidar2cam(lidar_in_cam,camera_intrinsic,img_raw_size):\n",
        "  ### Filter out data behind the cam\n",
        "  lidar_in_cam = np.concatenate((lidar_in_cam,np.ones((lidar_in_cam.shape[0],1))),axis=1)\n",
        "  lidar_in_cam = lidar_in_cam[lidar_in_cam[:,2]>0]\n",
        "\n",
        "  ### Project points to the image\n",
        "  lidar_2d = np.matmul(camera_intrinsic,lidar_in_cam[:,:3].T).T\n",
        "  lidar_2d = np.divide(lidar_2d,lidar_2d[:,2].reshape((-1,1)))\n",
        "  lidar_2d = lidar_2d.astype(int)\n",
        "\n",
        "  ### Filter out points that are outside image frame\n",
        "  maskH = np.logical_and(lidar_2d[:,0]>=0,lidar_2d[:,0]<img_raw_size[1])\n",
        "  maskV = np.logical_and(lidar_2d[:,1]>=0,lidar_2d[:,1]<img_raw_size[0])\n",
        "  mask = np.logical_and(maskH,maskV)\n",
        "  lidar_2d = lidar_2d[mask,:]\n",
        "  lidar_in_cam = lidar_in_cam[mask,:]\n",
        "\n",
        "  return lidar_2d,lidar_in_cam[:,:3]\n",
        "\n",
        "lidar_2d,lidar_in_cam = project_lidar2cam(lidar_raw,camera_intrinsic,img_raw_size)\n",
        "lidar_2d1,lidar_in_cam1 = project_lidar2cam(lidar_raw1,camera_intrinsic,img_raw_size)\n",
        "\n",
        "### Visualize\n",
        "print(f'Original image size: {img_raw.shape}')\n",
        "\n",
        "img = img_raw.copy()\n",
        "axs = 2\n",
        "axs_log = np.log(lidar_in_cam[:,axs]-np.min(lidar_in_cam[:,axs])+1)\n",
        "max_axs = np.max(axs_log)\n",
        "for pt,z in zip(lidar_2d,axs_log):\n",
        "    color_z = z*255/max_axs\n",
        "    c = (color_z,0,0)\n",
        "    cv2.circle(img,tuple(pt[:2].astype(int)),1,c,-1)\n",
        "\n",
        "img1 = img_raw1.copy()\n",
        "axs = 2\n",
        "axs_log1 = np.log(lidar_in_cam1[:,axs]-np.min(lidar_in_cam1[:,axs])+1)\n",
        "max_axs1 = np.max(axs_log1)\n",
        "for pt,z in zip(lidar_2d1,axs_log1):\n",
        "    color_z = z*255/max_axs1\n",
        "    c = (color_z,0,0)\n",
        "    cv2.circle(img1,tuple(pt[:2].astype(int)),1,c,-1)\n",
        "\n",
        "fig,axs = plt.subplots(2,1,figsize=(15,7))\n",
        "axs[0].imshow(img)\n",
        "axs[1].imshow(img1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1aTnCvzTGGO-"
      },
      "source": [
        "## Image Cropping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ya3jzuZL7GzM"
      },
      "source": [
        "The deep learning model (DeepLab v3+) was trained with cropped images from KITTI dataset with ratio 4:3 (W:H), which was resized further to 513 x 513 images. It will work better if we use the same size of images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7aD86U97LyP"
      },
      "source": [
        "### Function to crop the image according to the target size ratio\n",
        "### The image will be cropped at the center\n",
        "def crop_data(img_in,lidar_2d_in,lidar_in_cam_in,rh,rw):\n",
        "  lidar_2d = lidar_2d_in.copy()\n",
        "  lidar_in_cam = lidar_in_cam_in.copy()\n",
        "  img = img_in.copy()\n",
        "\n",
        "  ### Crop the image\n",
        "  dim_ori = np.array(img.shape)\n",
        "  cent = (dim_ori/2).astype(int)\n",
        "  if dim_ori[0]/dim_ori[1] == rh/rw:\n",
        "      crop_img = img\n",
        "    \n",
        "  # If Height <= Width\n",
        "  elif dim_ori[0] <= dim_ori[1]:\n",
        "      cH2 = dim_ori[0]\n",
        "      cW2 = cH2*rw/rh\n",
        "      cW = int(cW2/2)\n",
        "      crop_img = img[:,cent[1]-cW:cent[1]+cW+1]\n",
        "\n",
        "  # If Height > Width\n",
        "  else:\n",
        "      cW2 = dim_ori[1]\n",
        "      cH2 = cW2*rh/rw\n",
        "      cH = int(cH2/2)\n",
        "      crop_img = img[cent[0]-cH:cent[0]+cH+1,:]\n",
        "\n",
        "  ### Filter out LiDAR points outside cropped image\n",
        "  cW = cW2/2\n",
        "  cH = cH2/2\n",
        "  centH = cent[0]\n",
        "  centW = cent[1]\n",
        "  maskH = np.logical_and(lidar_2d[:,1]>=centH-cH,lidar_2d[:,1]<=centH+cH)\n",
        "  maskW = np.logical_and(lidar_2d[:,0]>=centW-cW,lidar_2d[:,0]<=centW+cW)\n",
        "  mask = np.logical_and(maskH,maskW)\n",
        "  lidar_2d = lidar_2d[mask,:]\n",
        "  lidar_in_cam = lidar_in_cam[mask,:]\n",
        "  cent = np.array((centW-cW,centH-cH,0)).reshape((1,3))\n",
        "  lidar_2d = lidar_2d - cent\n",
        "\n",
        "  return crop_img, lidar_2d.astype(int), lidar_in_cam\n",
        "\n",
        "### Cropped image's size ratio\n",
        "CROP_RH = 3 # Height ratio\n",
        "CROP_RW = 4 # Width ratio\n",
        "crop_img,lidar_2d,lidar_in_cam = crop_data(img_raw,lidar_2d,lidar_in_cam,CROP_RH,CROP_RW)\n",
        "crop_img1,lidar_2d1,lidar_in_cam1 = crop_data(img_raw1,lidar_2d1,lidar_in_cam1,CROP_RH,CROP_RW)\n",
        "\n",
        "### Visualize\n",
        "img = crop_img.copy()\n",
        "axs = 2\n",
        "axs_log = np.log(lidar_in_cam[:,axs]-np.min(lidar_in_cam[:,axs])+1)\n",
        "max_axs = np.max(axs_log)\n",
        "for pt,z in zip(lidar_2d,axs_log):\n",
        "    color_z = z*255/max_axs\n",
        "    c = (color_z,0,0)\n",
        "    cv2.circle(img,tuple(pt[:2].astype(int)),1,c,-1)\n",
        "\n",
        "img1 = crop_img1.copy()\n",
        "axs = 2\n",
        "axs_log1 = np.log(lidar_in_cam1[:,axs]-np.min(lidar_in_cam1[:,axs])+1)\n",
        "max_axs1 = np.max(axs_log1)\n",
        "for pt,z in zip(lidar_2d1,axs_log1):\n",
        "    color_z = z*255/max_axs1\n",
        "    c = (color_z,0,0)\n",
        "    cv2.circle(img1,tuple(pt[:2].astype(int)),1,c,-1)\n",
        "\n",
        "fig,axs = plt.subplots(1,2,figsize=(10,7))\n",
        "axs[0].imshow(img)\n",
        "axs[1].imshow(img1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhy-xR-xKPu6"
      },
      "source": [
        "## Road Segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxb-TFz0ATMJ"
      },
      "source": [
        "Process the camera image with image segmentation-based deep learning method to get regions that correspond to the road. Specifically, we use DeepLab v3+ model that has been trained before with KITTI dataset. [Read this article](https://rockyshikoku.medium.com/train-deeplab-v3-with-your-own-dataset-13f2af958a75) if you want to know how to train your own DeepLab v3+ model. \n",
        "\n",
        "Now, by projecting the LiDAR points to the segmented image, we can know which points that correspond to the road. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vM6rRV1h733l"
      },
      "source": [
        "### Function to process the image with DeepLabv3+\n",
        "def process_images(img_in, sess, target_size=513, probability_threshold=0.5):\n",
        "  INPUT_TENSOR_NAME = 'ImageTensor:0'\n",
        "  PROB_TENSOR_NAME = 'SemanticProbabilities:0'\n",
        "  INPUT_SIZE = target_size\n",
        "\n",
        "  image = img_in.copy()\n",
        "  sz = image.shape\n",
        "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "  # Resize input image to target size\n",
        "  if INPUT_SIZE == 0:\n",
        "    resized_image = image.copy()\n",
        "  else:\n",
        "    resized_image = cv2.resize(image,(INPUT_SIZE,INPUT_SIZE))\n",
        "\n",
        "  # Run deep learning inference\n",
        "  batch_seg_map = sess.run(\n",
        "      PROB_TENSOR_NAME,\n",
        "      feed_dict={INPUT_TENSOR_NAME: [np.asarray(resized_image)]})\n",
        "  seg_map = (batch_seg_map[0][:,:,1]*255).astype(int)\n",
        "  prob = np.array(seg_map, dtype=np.uint8)\n",
        "  prob = cv2.resize(prob,(sz[1],sz[0]))\n",
        "\n",
        "  # Create the prediction\n",
        "  pred = prob.copy()\n",
        "  msk_bin = prob >= (probability_threshold*255)\n",
        "  pred[msk_bin] = 1\n",
        "  pred[np.logical_not(msk_bin)] = 0\n",
        "\n",
        "  # Ignore regions that are separated from the main road\n",
        "  # This can reduce the amount of false detection\n",
        "  _,segm_reg = cv2.connectedComponents(pred)\n",
        "  segm_reg = segm_reg.astype(float)\n",
        "  segm_reg[segm_reg==0] = np.nan\n",
        "  modes,_ = stats.mode(segm_reg,axis=None)\n",
        "  mode = modes[0]\n",
        "  pred[segm_reg!=mode] = 0\n",
        "  \n",
        "  return prob,(pred*255).astype(np.uint8)\n",
        "\n",
        "### Load the model\n",
        "model_path = 'KITTI_Mapping/pretrained/deeplab_model.pb'\n",
        "with open(model_path, \"rb\") as f:\n",
        "    graph_def = tf.compat.v1.GraphDef.FromString(f.read())\n",
        "graph = tf.Graph()\n",
        "with graph.as_default():\n",
        "    tf.import_graph_def(graph_def=graph_def, name=\"\")\n",
        "sess = tf.compat.v1.Session(graph=graph)\n",
        "\n",
        "target_size = 513\n",
        "segm_prob,segm_pred = process_images(crop_img, sess, target_size, 0.5)\n",
        "segm_prob1,segm_pred1 = process_images(crop_img1, sess, target_size, 0.5)\n",
        "\n",
        "### Visualize\n",
        "segm_3ch = crop_img.copy()\n",
        "segm_3ch[:,:,0] = segm_prob\n",
        "segm_3ch[:,:,1] = segm_prob\n",
        "segm_3ch[:,:,2] = segm_prob\n",
        "for pt in lidar_2d:\n",
        "  pt = pt.astype(int)\n",
        "  if segm_pred[pt[1],pt[0]] == 0:\n",
        "    continue\n",
        "  c = (255,0,0)\n",
        "  cv2.circle(segm_3ch,tuple(pt[:2]),1,c,-1)\n",
        "\n",
        "segm_3ch1 = crop_img1.copy()\n",
        "segm_3ch1[:,:,0] = segm_prob1\n",
        "segm_3ch1[:,:,1] = segm_prob1\n",
        "segm_3ch1[:,:,2] = segm_prob1\n",
        "for pt in lidar_2d1:\n",
        "  pt = pt.astype(int)\n",
        "  if segm_pred1[pt[1],pt[0]] == 0:\n",
        "    continue\n",
        "  c = (255,0,0)\n",
        "  cv2.circle(segm_3ch1,tuple(pt[:2]),1,c,-1)\n",
        "\n",
        "fig,axs = plt.subplots(1,2,figsize=(10,7))\n",
        "axs[0].imshow(segm_3ch)\n",
        "axs[1].imshow(segm_3ch1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5HB34TQc8bC"
      },
      "source": [
        "## LiDAR Road Filter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2BBdmSkA5-4"
      },
      "source": [
        "The road points set that are obtained from the previous step may contain false detection and obviously only contains point that are inside the camera field of view. We also need to detect road points that are outside this set!\n",
        "\n",
        "To do this, fit a plane model (which will represent the road model) to the current road points. Any points that are outside the camera field of view and located near the road plane model can be also regarded as road points. RANSAC algorithm is applied when fitting the model to reduce the influence of outliers. Visit this [Wikipedia page](https://en.wikipedia.org/wiki/Random_sample_consensus) for more information about RANSAC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKocWSZ9dG-D"
      },
      "source": [
        "### Get the plane model from the road points\n",
        "def get_road_model_ransac(img_pred,lidar_in_cam,lidar_2d):\n",
        "  lidar_in_road_lbl = [True if img_pred[pt[1],pt[0]] == 255 else False for pt in lidar_2d]\n",
        "  lidar_in_road = lidar_in_cam[lidar_in_road_lbl,:]\n",
        "  road_model = RANSACRegressor().fit(lidar_in_road[:,[0,2]],lidar_in_road[:,1])\n",
        "  return road_model\n",
        "\n",
        "### Predict all LiDAR points as road or not\n",
        "def filter_road_points(road_model,lidar_in,threshold=0.5):\n",
        "  x = lidar_in[:,[0,2]]\n",
        "  y_true = lidar_in[:,1]\n",
        "  y_pred = road_model.predict(x)\n",
        "  delta_y = np.absolute(y_true-y_pred).flatten()\n",
        "  is_not_road = delta_y > threshold\n",
        "  lidar_out = lidar_in[is_not_road,:].copy()\n",
        "  return lidar_out\n",
        "\n",
        "road_height_threshold = 0.1\n",
        "\n",
        "road_model = get_road_model_ransac(segm_pred,lidar_in_cam,lidar_2d)\n",
        "lidar_nonroad = filter_road_points(road_model,lidar_raw,road_height_threshold)\n",
        "\n",
        "road_model1 = get_road_model_ransac(segm_pred1,lidar_in_cam1,lidar_2d1)\n",
        "lidar_nonroad1 = filter_road_points(road_model1,lidar_raw1,road_height_threshold)\n",
        "\n",
        "### Visualize\n",
        "fig,axs = plt.subplots(1,2,figsize=(20,10))\n",
        "axs[0].scatter(lidar_nonroad[:,0],lidar_nonroad[:,2],c=-lidar_nonroad[:,1],marker='.')\n",
        "axs[0].scatter(0,0,c='r',marker='x')\n",
        "axs[0].axis('scaled')\n",
        "axs[1].scatter(lidar_nonroad1[:,0],lidar_nonroad1[:,2],c=-lidar_nonroad1[:,1],marker='.')\n",
        "axs[1].scatter(0,0,c='r',marker='x')\n",
        "axs[1].axis('scaled')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEJENqREB-xe"
      },
      "source": [
        "Now we can remove the road points from the LiDAR data and proceed to the mapping system."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvjFQgbURjkh"
      },
      "source": [
        "# MAPPING (OGM)\n",
        "\n",
        "The occupancy grid map "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84BWoo6ft0Pc"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_H-xY32t151"
      },
      "source": [
        "ALPHA = 1\n",
        "BHETA = 1*np.pi/180\n",
        "RESOLUTION = 0.1\n",
        "MAX_RANGE = 50\n",
        "MAP_WIDTH = 100\n",
        "SPHERICAL2CARTESIAN_BIAS = 6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwA7TdDCtnL8"
      },
      "source": [
        "## Map Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNTcnvRbtmeD"
      },
      "source": [
        "MAP_SIZE_X = int(MAP_WIDTH/RESOLUTION)\n",
        "MAP_SIZE_Y = int(MAP_WIDTH/RESOLUTION)\n",
        "ogm_time_0 = np.ones((MAP_SIZE_Y,MAP_SIZE_X)) * 0.5\n",
        "\n",
        "xarr = np.arange(-MAP_WIDTH/2,MAP_WIDTH/2,RESOLUTION)\n",
        "yarr = np.arange(-MAP_WIDTH/2,MAP_WIDTH/2,RESOLUTION)\n",
        "MAP_XX, MAP_YY = np.meshgrid(xarr, -yarr)\n",
        "rgrid = np.sqrt(np.add(np.square(MAP_XX),np.square(MAP_YY)))\n",
        "OOR_MASK = rgrid >= MAX_RANGE\n",
        "\n",
        "lidar_ogm = lidar_nonroad[:,[2,0]]\n",
        "lidar_ogm1 = lidar_nonroad1[:,[2,0]]\n",
        "\n",
        "plt.figure(figsize=(6, 6))\n",
        "plt.imshow(ogm_time_0,cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgO7kAzqz8Av"
      },
      "source": [
        "## Generate Measurement Grid from Filtered LiDAR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqcwHASlRmI9"
      },
      "source": [
        "def generate_measurement_ogm(lidar_in,ogm_shape):\n",
        "  rphi_meas = np.zeros((lidar_in.shape[0],2))\n",
        "  rphi_meas[:,1] = np.sqrt(np.add(np.square(lidar_in[:,0]),np.square(lidar_in[:,1])))/ALPHA\n",
        "  rphi_meas[:,0] = (np.arctan2(lidar_in[:,1],lidar_in[:,0])+np.pi)/BHETA\n",
        "  rphi_meas = np.unique(rphi_meas.astype(int),axis=0)\n",
        "  rphi_meas = rphi_meas[rphi_meas[:,1]<int(MAX_RANGE/ALPHA),:]\n",
        "  rphi_meas = rphi_meas[rphi_meas[:,0]<int(2*np.pi/BHETA),:]\n",
        "\n",
        "  sg_ang_bin = int(2*np.pi/BHETA)\n",
        "  sg_rng_bin = int(MAX_RANGE/ALPHA)\n",
        "  scan_grid = np.ones((sg_ang_bin,sg_rng_bin))*0.5\n",
        "  scan_grid[tuple(rphi_meas.T)] = 0.7\n",
        "  \n",
        "  for ang in range(sg_ang_bin):\n",
        "    ang_arr = rphi_meas[rphi_meas[:,0]==ang,1]\n",
        "    if len(ang_arr) == 0:\n",
        "      scan_grid[ang,:] = 0.3\n",
        "    else:\n",
        "      min_r = np.min(ang_arr)\n",
        "      scan_grid[ang,:min_r] = 0.3\n",
        "  \n",
        "  ogm_sz = (ogm_shape[1],ogm_shape[0])\n",
        "  ogm_cen = (int(ogm_shape[1]/2),int(ogm_shape[0]/2))\n",
        "  radius = (MAX_RANGE/RESOLUTION) + SPHERICAL2CARTESIAN_BIAS\n",
        "  ogm_step = cv2.warpPolar(scan_grid,ogm_sz,ogm_cen,radius,cv2.WARP_INVERSE_MAP)\n",
        "  ogm_step[OOR_MASK] = 0.5\n",
        "  ogm_step = cv2.rotate(ogm_step, cv2.ROTATE_90_CLOCKWISE)\n",
        "  return ogm_step\n",
        "\n",
        "ogm_step = generate_measurement_ogm(lidar_ogm,ogm_time_0.shape)\n",
        "ogm_step1 = generate_measurement_ogm(lidar_ogm1,ogm_time_0.shape)\n",
        "\n",
        "### Visualize\n",
        "fig,axs = plt.subplots(1,2,figsize=(12,6))\n",
        "axs[0].imshow(((1-ogm_step)*255).astype(np.uint8),cmap='gray')\n",
        "axs[1].imshow(((1-ogm_step1)*255).astype(np.uint8),cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZzVVZF47bRb"
      },
      "source": [
        "## Try First Update"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VNGMlHzav7-y"
      },
      "source": [
        "def logit(m):\n",
        "  return np.log(np.divide(m, np.subtract(1, m)))\n",
        "\n",
        "def inverse_logit(m):\n",
        "  return np.divide(np.exp(m),np.add(1,np.exp(m)))\n",
        "\n",
        "def update_ogm(prior_ogm,new_ogm):\n",
        "  logit_map = logit(new_ogm) + logit(prior_ogm)\n",
        "  out_ogm = inverse_logit(logit_map)\n",
        "  out_ogm[out_ogm>=0.98] = 0.98\n",
        "  out_ogm[out_ogm<=0.02] = 0.02\n",
        "  return out_ogm\n",
        "\n",
        "ogm_time_1 = update_ogm(ogm_step,ogm_time_0)\n",
        "\n",
        "### Visualize\n",
        "fig,axs = plt.subplots(1,1,figsize=(6,6))\n",
        "axs.imshow(((1-ogm_time_1)*255).astype(np.uint8),cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKHWHNSzIhTR"
      },
      "source": [
        "## Load Vehicle's Poses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaiHzx5ABM2L"
      },
      "source": [
        "### Load the vehicle's pose based on velocity data\n",
        "def load_vehicle_pose_vel(data,idx,old_pose,old_idx):\n",
        "  delta_t = (data.timestamps[idx]-data.timestamps[old_idx]).total_seconds()\n",
        "  packet = data.oxts[idx].packet\n",
        "  vf = packet.vf\n",
        "  vr = -packet.vl\n",
        "  pose_f = old_pose[0] + (vf*delta_t)\n",
        "  pose_r = old_pose[1] + (vr*delta_t)\n",
        "  pose_y = packet.yaw - data.oxts[0].packet.yaw\n",
        "  return (pose_f,pose_r,pose_y)\n",
        "\n",
        "\n",
        "pose = load_vehicle_pose_vel(data,idx,(0,0,0),idx)\n",
        "pose1 = load_vehicle_pose_vel(data,idx1,pose,idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrJZJfSx8UXT"
      },
      "source": [
        "## Shift the OGM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_2GHyEr1YV4"
      },
      "source": [
        "# Shift the map according the vehicle's poses\n",
        "def shift_pose_ogm(ogm, init, fin):\n",
        "  ogm_o = ogm.copy()\n",
        "  theta = init[2] /180 * np.pi\n",
        "  rot_m = np.array([[np.cos(theta),np.sin(theta)],[-np.sin(theta),np.cos(theta)]])\n",
        "  trs_m = np.array([[init[0]],[init[1]]])\n",
        "  point = np.array(fin[:2]).reshape((-1,1))\n",
        "  point_1 = (point - trs_m)\n",
        "  point_2 = np.dot(rot_m,-point_1)\n",
        "  delta_theta = (fin[2] - init[2])\n",
        "  delta = np.array([point_2[1,0]/RESOLUTION,point_2[0,0]/RESOLUTION,0])\n",
        "\n",
        "  M = np.array([[1,0,delta[0]],[0,1,-delta[1]]])\n",
        "  dst = cv2.warpAffine(ogm_o,M,(ogm_o.shape[1],ogm_o.shape[0]),borderValue=0.5)\n",
        "  M = cv2.getRotationMatrix2D((ogm_o.shape[1]/2+0.5,ogm_o.shape[0]/2+0.5),delta_theta,1)\n",
        "  dst = cv2.warpAffine(dst,M,(ogm_o.shape[1],ogm_o.shape[0]),borderValue=0.5)\n",
        "  return dst\n",
        "\n",
        "shift_ogm_time_1 = shift_pose_ogm(ogm_time_1,pose,pose1)\n",
        "\n",
        "### Visualize\n",
        "fig,axs = plt.subplots(1,2,figsize=(12,6))\n",
        "axs[0].imshow(((1-ogm_time_1)*255).astype(np.uint8),cmap='gray')\n",
        "axs[0].set_title('Before')\n",
        "axs[1].imshow(((1-shift_ogm_time_1)*255).astype(np.uint8),cmap='gray')\n",
        "axs[1].set_title('After')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NsVJKtwH1fg"
      },
      "source": [
        "## Try Second Update"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EG1zM2g19Ffo"
      },
      "source": [
        "ogm_time_2 = update_ogm(ogm_step1,shift_ogm_time_1)\n",
        "\n",
        "### Visualize\n",
        "fig,axs = plt.subplots(1,1,figsize=(6,6))\n",
        "axs.imshow(((1-ogm_time_2)*255).astype(np.uint8),cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2ThKoH26A_D"
      },
      "source": [
        "# MAPPING (DGM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAJb3jyM6RSt"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XYbJZnS6RSt"
      },
      "source": [
        "ALPHA = 1\n",
        "BHETA = 1*np.pi/180\n",
        "RESOLUTION = 0.1\n",
        "MAX_RANGE = 50\n",
        "MAP_WIDTH = 100\n",
        "SPHERICAL2CARTESIAN_BIAS = 6\n",
        "FREE_CONF = 0.7\n",
        "OCC_CONF = 0.7\n",
        "DYNAMIC_THRESHOLD = 0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrmbUStU6RSt"
      },
      "source": [
        "## Map Initialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z30qPjM66RSt"
      },
      "source": [
        "'''\n",
        "DGM Channel:\n",
        "Channel 0 = {F,O}\n",
        "Channel 1 = O\n",
        "Channel 2 = F\n",
        "'''\n",
        "MAP_SIZE_X = int(MAP_WIDTH/RESOLUTION)\n",
        "MAP_SIZE_Y = int(MAP_WIDTH/RESOLUTION)\n",
        "dgm_time_0 = np.zeros((MAP_SIZE_Y,MAP_SIZE_X,3))\n",
        "dgm_time_0[:,:,0] = 1 \n",
        "\n",
        "xarr = np.arange(-MAP_WIDTH/2,MAP_WIDTH/2,RESOLUTION)\n",
        "yarr = np.arange(-MAP_WIDTH/2,MAP_WIDTH/2,RESOLUTION)\n",
        "MAP_XX, MAP_YY = np.meshgrid(xarr, -yarr)\n",
        "rgrid = np.sqrt(np.add(np.square(MAP_XX),np.square(MAP_YY)))\n",
        "OOR_MASK = rgrid >= MAX_RANGE\n",
        "\n",
        "lidar_dgm = lidar_nonroad[:,[2,0]]\n",
        "lidar_dgm1 = lidar_nonroad1[:,[2,0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZhB3ouU6RSu"
      },
      "source": [
        "## Generate Measurement Grid from Filtered LiDAR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HT0gy9BM6RSu"
      },
      "source": [
        "def generate_measurement_dgm(lidar_in,dgm_shape):\n",
        "  rphi_meas = np.zeros((lidar_in.shape[0],2))\n",
        "  rphi_meas[:,1] = np.sqrt(np.add(np.square(lidar_in[:,0]),np.square(lidar_in[:,1])))/ALPHA\n",
        "  rphi_meas[:,0] = (np.arctan2(lidar_in[:,1],lidar_in[:,0])+np.pi)/BHETA\n",
        "  rphi_meas = np.unique(rphi_meas.astype(int),axis=0)\n",
        "  rphi_meas = rphi_meas[rphi_meas[:,1]<int(MAX_RANGE/ALPHA),:]\n",
        "  rphi_meas = rphi_meas[rphi_meas[:,0]<int(2*np.pi/BHETA),:]\n",
        "\n",
        "  sg_ang_bin = int(2*np.pi/BHETA)\n",
        "  sg_rng_bin = int(MAX_RANGE/ALPHA)\n",
        "  scan_grid = np.zeros((sg_ang_bin,sg_rng_bin,3))\n",
        "  scan_grid[:,:,0] = 1 \n",
        "  scan_grid[tuple(rphi_meas.T)] = (1-OCC_CONF,OCC_CONF,0)\n",
        "  \n",
        "  for ang in range(sg_ang_bin):\n",
        "    ang_arr = rphi_meas[rphi_meas[:,0]==ang,1]\n",
        "    if len(ang_arr) == 0:\n",
        "      scan_grid[ang,:] = (1-FREE_CONF,0,FREE_CONF)\n",
        "    else:\n",
        "      min_r = np.min(ang_arr)\n",
        "      scan_grid[ang,:min_r] = (1-FREE_CONF,0,FREE_CONF)\n",
        "  \n",
        "  dgm_sz = (dgm_shape[1],dgm_shape[0])\n",
        "  dgm_cen = (int(dgm_shape[1]/2),int(dgm_shape[0]/2))\n",
        "  radius = (MAX_RANGE/RESOLUTION) + SPHERICAL2CARTESIAN_BIAS\n",
        "  dgm_step = cv2.warpPolar(scan_grid,dgm_sz,dgm_cen,radius,cv2.WARP_INVERSE_MAP)\n",
        "  dgm_step[OOR_MASK] = (1,0,0)\n",
        "  dgm_step = cv2.rotate(dgm_step, cv2.ROTATE_90_CLOCKWISE)\n",
        "  return dgm_step\n",
        "\n",
        "dgm_step = generate_measurement_dgm(lidar_dgm,dgm_time_0.shape)\n",
        "dgm_step1 = generate_measurement_dgm(lidar_dgm1,dgm_time_0.shape)\n",
        "\n",
        "### Visualize\n",
        "fig,axs = plt.subplots(1,2,figsize=(12,6))\n",
        "axs[0].imshow((dgm_step*255).astype(np.uint8))\n",
        "axs[1].imshow((dgm_step1*255).astype(np.uint8))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9J6VTpplV_u"
      },
      "source": [
        "## Try First Update"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mo5JbqzICarz"
      },
      "source": [
        "def update_dgm(prior_dgm,new_dgm):\n",
        "  conflict_mass = np.multiply(prior_dgm[:,:,2],new_dgm[:,:,1])\n",
        "  conflict_mass = np.add(conflict_mass,np.multiply(prior_dgm[:,:,1],new_dgm[:,:,2]))\n",
        "\n",
        "  free_mass = np.multiply(prior_dgm[:,:,0],new_dgm[:,:,2])\n",
        "  free_mass = np.add(free_mass,np.multiply(prior_dgm[:,:,2],new_dgm[:,:,0]))\n",
        "  free_mass = np.add(free_mass,np.multiply(prior_dgm[:,:,2],new_dgm[:,:,2]))\n",
        "  free_mass = np.divide(free_mass,1-conflict_mass)\n",
        "\n",
        "  occ_mass = np.multiply(prior_dgm[:,:,0],new_dgm[:,:,1])\n",
        "  occ_mass = np.add(occ_mass,np.multiply(prior_dgm[:,:,1],new_dgm[:,:,0]))\n",
        "  occ_mass = np.add(occ_mass,np.multiply(prior_dgm[:,:,1],new_dgm[:,:,1]))\n",
        "  occ_mass = np.divide(occ_mass,1-conflict_mass)\n",
        "\n",
        "  unknown_mass = np.multiply(prior_dgm[:,:,0],new_dgm[:,:,0])\n",
        "  unknown_mass = np.divide(unknown_mass,1-conflict_mass)\n",
        "\n",
        "  updated_dgm = np.stack((unknown_mass,occ_mass,free_mass),axis=2)\n",
        "  return updated_dgm,conflict_mass\n",
        "\n",
        "def predict_dgm(dgm,dynamic_mass):\n",
        "  max_mass = np.argmax(dgm,axis=2)\n",
        "  pred_map = np.zeros(dgm.shape)\n",
        "  pred_map[max_mass==0] = (123,123,123)\n",
        "  pred_map[max_mass==1] = (0,0,0)\n",
        "  pred_map[max_mass==2] = (255,255,255)\n",
        "  pred_map[dynamic_mass>=DYNAMIC_THRESHOLD] = (0,0,255)\n",
        "  return pred_map.astype(np.uint8)\n",
        "\n",
        "dgm_time_1,dynamic_mass_1 = update_dgm(dgm_time_0,dgm_step)\n",
        "dgm_pred_1 = predict_dgm(dgm_time_1,dynamic_mass_1)\n",
        "\n",
        "fig,axs = plt.subplots(1,1,figsize=(6,6))\n",
        "axs.imshow(dgm_pred_1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VItwuWo2ljyK"
      },
      "source": [
        "## Load Vehicle's Pose"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3A3tq0OkjlB"
      },
      "source": [
        "### Load the vehicle's pose based on velocity data\n",
        "def load_vehicle_pose_vel(data,idx,old_pose,old_idx):\n",
        "  delta_t = (data.timestamps[idx]-data.timestamps[old_idx]).total_seconds()\n",
        "  packet = data.oxts[idx].packet\n",
        "  vf = packet.vf\n",
        "  vr = -packet.vl\n",
        "  pose_f = old_pose[0] + (vf*delta_t)\n",
        "  pose_r = old_pose[1] + (vr*delta_t)\n",
        "  pose_y = packet.yaw - data.oxts[0].packet.yaw\n",
        "  return (pose_f,pose_r,pose_y)\n",
        "\n",
        "\n",
        "pose = load_vehicle_pose_vel(data,idx,(0,0,0),idx)\n",
        "pose1 = load_vehicle_pose_vel(data,idx1,pose,idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYeJ8xxUlo_J"
      },
      "source": [
        "## Shift the DGM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwgvCoqqliyJ"
      },
      "source": [
        "# Shift the map according the vehicle's poses\n",
        "def shift_pose_dgm(dgm, init, fin):\n",
        "  dgm_o = dgm.copy()\n",
        "  theta = init[2] /180 * np.pi\n",
        "  rot_m = np.array([[np.cos(theta),np.sin(theta)],[-np.sin(theta),np.cos(theta)]])\n",
        "  trs_m = np.array([[init[0]],[init[1]]])\n",
        "  point = np.array(fin[:2]).reshape((-1,1))\n",
        "  point_1 = (point - trs_m)\n",
        "  point_2 = np.dot(rot_m,-point_1)\n",
        "  delta_theta = (fin[2] - init[2])\n",
        "  delta = np.array([point_2[1,0]/RESOLUTION,point_2[0,0]/RESOLUTION,0])\n",
        "\n",
        "  M = np.array([[1,0,delta[0]],[0,1,-delta[1]]])\n",
        "  dst = cv2.warpAffine(dgm_o,M,(dgm_o.shape[1],dgm_o.shape[0]),borderValue=0.5)\n",
        "  M = cv2.getRotationMatrix2D((dgm_o.shape[1]/2+0.5,dgm_o.shape[0]/2+0.5),delta_theta,1)\n",
        "  dst = cv2.warpAffine(dst,M,(dgm_o.shape[1],dgm_o.shape[0]),borderValue=0.5)\n",
        "  return dst\n",
        "\n",
        "shift_dgm_time_1 = shift_pose_dgm(dgm_time_1,pose,pose1)\n",
        "\n",
        "### Visualize\n",
        "fig,axs = plt.subplots(1,2,figsize=(12,6))\n",
        "axs[0].imshow(((1-ogm_time_1)*255).astype(np.uint8),cmap='gray')\n",
        "axs[0].set_title('Before')\n",
        "axs[1].imshow(((1-shift_ogm_time_1)*255).astype(np.uint8),cmap='gray')\n",
        "axs[1].set_title('After')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LWBhx2Yl_-k"
      },
      "source": [
        "## The Second Update"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuJUZl4Zl8fI"
      },
      "source": [
        "dgm_time_2,dynamic_mass_2 = update_dgm(shift_dgm_time_1,dgm_step1)\n",
        "dgm_pred_2 = predict_dgm(dgm_time_2,dynamic_mass_2)\n",
        "\n",
        "fig,axs = plt.subplots(1,1,figsize=(6,6))\n",
        "axs.imshow(dgm_pred_2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6laW_VD5mYBn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}